{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter extraction\n",
    "\n",
    "This notebook aims to retrieve tweets, clean them and compute a sentiment in order to observe a correlation between crypto currencies and tweets' sentiments. The following steps are executed in this notebook :\n",
    "\n",
    "- Retrieve tweets with Twython API (Twitter API wrapper for python)\n",
    "- Extract the wanted data (tweet's text, #followers, #likes, etc.)\n",
    "- Clean the textual data (remove unnecessary elements like media, websites link, pseudos, ...)\n",
    "- Compute for each tweet a sentiment score with Vader (named compound) and a score linked to the popularity of the tweet and its compound\n",
    "\n",
    "This notebook is written using Python 3.6.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the currency\n",
    "CURRENCY = \"\" # Enter the name of the name of the curruncy e.g (cardano,bitcoin,dogecoin)\n",
    "CURRENCY_SYMBOL = \"\" # Name of the curruncy symbol e.g (ADA, BTC, D)\n",
    "\n",
    "## personal config\n",
    "TWEETS_FOLDER    = \"data/crypto/%s\"%(CURRENCY) # Relative path to historical data\n",
    "SEP_CHAR         = '~' # character seperating dates from and to in filename\n",
    "ENVS             = ['CRYPTO', 'LINE_COUNT', 'MOST_RECENT_FILE', 'MOST_RECENT_ID'] # Stored in var.csv\n",
    "MAX_ROW_PER_FILE = 20000 # Each file storing data has a maximum amount of rows\n",
    "\n",
    "tweets_raw_file = 'data/twitter/%s/%s_tweets_raw.csv'%(CURRENCY_SYMBOL,CURRENCY)\n",
    "tweets_clean_file = 'data/twitter/%s/%s_tweets_clean.csv'%(CURRENCY_SYMBOL,CURRENCY)\n",
    "query = '#%s OR #%s'%(CURRENCY,CURRENCY_SYMBOL) ####TODO PUT BACK  OR {CURRENCY} OR ${CURRENCY} OR ${CURRENCY_SYMBOL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieve the tweets from Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Twython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 OAuth2 Authentication (*app* authentication)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/search/tweets': {'limit': 450, 'remaining': 450, 'reset': 1628404812}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_KEY =''  # Enter your API key\n",
    "APP_SECRET =  '' # Enter API Secret key\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "twitter.get_application_rate_limit_status()['resources']['search']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Query the twitter API\n",
    "Here we query the twitter API to get the latest tweets about Cardano. Then we transform it to store only the useful data inside a Pandas Dataframe.\n",
    "\n",
    "The following fields are retrieved from the response:\n",
    "\n",
    "- **id** (int) : unique identifier of the tweet\n",
    "- **text** (string) : UTF-8 textual content of the tweet, max 140 chars\n",
    "- user\n",
    "  - **name** (string) : twitter's pseudo of the user\n",
    "  - **followers_count** (int) : Number of followers the user has\n",
    "- **retweet_count** (int) : Number of times the tweet has been retweeted\n",
    "- **favorite_count** (int) : Number of likes\n",
    "- **created_at** (datetime) : creation date and time of the tweet\n",
    "\n",
    "\n",
    "The pandas package must be installed using *pip install pandas* from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from time import sleep\n",
    "import json\n",
    "import pandas as pd\n",
    "import io\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/twitter/ADA/cardano_tweets_raw.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_raw_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 450/450 [06:21<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 44706, waiting for 15 minutes until next queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 450/450 [06:31<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 43668, waiting for 15 minutes until next queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████████████████████████▎                                        | 221/450 [03:01<02:48,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 21843, waiting for 15 minutes until next queries\n",
      "No more new tweets, stopping...\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_QUERIES = 450\n",
    "data = {\"statuses\": []}\n",
    "next_id = \"\" #\"1147236962945961984\"\n",
    "since_id= ''\n",
    "with open(tweets_raw_file,\"a+\", encoding='utf-8') as f:\n",
    "    if not next_id and not since_id:\n",
    "        f.write(\"ID,Text,UserName,UserFollowerCount,RetweetCount,Likes,CreatedAt\\n\")\n",
    "    while(True):\n",
    "        twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "        last_size = 0\n",
    "        for i in tqdm(range(NUMBER_OF_QUERIES)):\n",
    "            if not next_id:\n",
    "                data = twitter.search(q=query, lang='en', result_type='recent', count=\"100\",tweet_mode='extended',since_id=since_id) # Use since_id for tweets after id\n",
    "#                 print(data)\n",
    "            elif since_id:\n",
    "                data[\"statuses\"].extend(twitter.search(q=query, lang='en', result_type='mixed', count=\"100\",max_id=next_id,tweet_mode='extended')[\"statuses\"])\n",
    "            else:\n",
    "                data[\"statuses\"].extend(twitter.search(q=query, lang='en', result_type='mixed', count=\"100\", max_id=next_id,tweet_mode='extended')[\"statuses\"])\n",
    "            if len(data[\"statuses\"]) > 1:\n",
    "                next_id = data[\"statuses\"][len(data[\"statuses\"]) - 1]['id']\n",
    "            if last_size + 1 == len(data[\"statuses\"]):\n",
    "                break\n",
    "            else:\n",
    "                last_size = len(data[\"statuses\"])\n",
    "\n",
    "        print('Retrieved {0}, waiting for 15 minutes until next queries'.format(len(data[\"statuses\"])))\n",
    "        d = pd.DataFrame([[s[\"id\"], s[\"full_text\"].replace('\\n','').replace('\\r',''), s[\"user\"][\"name\"], s[\"user\"][\"followers_count\"], s[\"retweet_count\"], s[\"favorite_count\"], s[\"created_at\"]] for s in data[\"statuses\"]], columns=('ID', 'Text', 'UserName', \"UserFollowerCount\", 'RetweetCount', 'Likes', \"CreatedAt\"))\n",
    "        d.to_csv(f, mode='a', encoding='utf-8',index=False,header=False)\n",
    "        if last_size + 1 == len(data[\"statuses\"]):\n",
    "            print('No more new tweets, stopping...')\n",
    "            break\n",
    "        data[\"statuses\"] = []\n",
    "        \n",
    "        sleep(910)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Now we will cleanup the data.\n",
    "\n",
    "We already filtered tweets in english in the call to the Twitter API.\n",
    "We will now filter links, @Pseudo, images, videos, unhashtag #happy -> happy.\n",
    "\n",
    "We won't transform to lower case because Vader take capital letters into consideration to emphasize sentiments.\n",
    "\n",
    "You must install `pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 110217/110217 [31:26<00:00, 58.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import re # regular expressions\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "\n",
    "d = pd.read_csv(tweets_raw_file)\n",
    "for i,s in enumerate(tqdm(d['Text'])):\n",
    "    text = d.loc[i, 'Text']\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = re.sub('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub('@\\\\w+ *', '', text, flags=re.MULTILINE)\n",
    "    d.loc[i, 'Text'] = text\n",
    "f = open(tweets_clean_file, 'a+', encoding='utf-8')\n",
    "d.to_csv(f, header=True, encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of firts notebok extracting the data and cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
